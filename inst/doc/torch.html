<!DOCTYPE html>
<html>
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">
<title>mirai - Torch Integration</title>
<style type="text/css">
/**
 * Prism.s theme ported from highlight.js's xcode style
 */
pre code {
  padding: 1em;
}
.token.comment {
  color: #007400;
}
.token.punctuation {
  color: #999;
}
.token.tag,
.token.selector {
  color: #aa0d91;
}
.token.boolean,
.token.number,
.token.constant,
.token.symbol {
  color: #1c00cf;
}
.token.property,
.token.attr-name,
.token.string,
.token.char,
.token.builtin {
  color: #c41a16;
}
.token.inserted {
  background-color: #ccffd8;
}
.token.deleted {
  background-color: #ffebe9;
}
.token.operator,
.token.entity,
.token.url,
.language-css .token.string,
.style .token.string {
  color: #9a6e3a;
}
.token.atrule,
.token.attr-value,
.token.keyword {
  color: #836c28;
}
.token.function,
.token.class-name {
  color: #DD4A68;
}
.token.regex,
.token.important,
.token.variable {
  color: #5c2699;
}
.token.important,
.token.bold {
  font-weight: bold;
}
.token.italic {
  font-style: italic;
}
</style>
<style type="text/css">
body {
  font-family: sans-serif;
  max-width: 800px;
  margin: auto;
  padding: 1em;
  line-height: 1.5;
  box-sizing: border-box;
}
body, .footnotes, code { font-size: .9em; }
li li { font-size: .95em; }
*, *:before, *:after {
  box-sizing: inherit;
}
pre, img { max-width: 100%; }
pre, pre:hover {
  white-space: pre-wrap;
  word-break: break-all;
}
pre code {
  display: block;
  overflow-x: auto;
}
code { font-family: 'DejaVu Sans Mono', 'Droid Sans Mono', 'Lucida Console', Consolas, Monaco, monospace; }
:not(pre) > code, code[class] { background-color: #F8F8F8; }
code.language-undefined, pre > code:not([class]) {
  background-color: inherit;
  border: 1px solid #eee;
}
table {
  margin: auto;
  border-top: 1px solid #666;
}
table thead th { border-bottom: 1px solid #ddd; }
th, td { padding: 5px; }
thead, tfoot, tr:nth-child(even) { background: #eee; }
blockquote {
  color: #666;
  margin: 0;
  padding-left: 1em;
  border-left: 0.5em solid #eee;
}
hr, .footnotes::before { border: 1px dashed #ddd; }
.frontmatter { text-align: center; }
#TOC .numbered li { list-style: none; }
#TOC .numbered { padding-left: 0; }
#TOC .numbered ul { padding-left: 1em; }
table, .body h2 { border-bottom: 1px solid #666; }
.body .appendix, .appendix ~ h2 { border-bottom-style: dashed; }
.footnote-ref a::before { content: "["; }
.footnote-ref a::after { content: "]"; }
section.footnotes::before {
  content: "";
  display: block;
  max-width: 20em;
}

@media print {
  body {
    font-size: 12pt;
    max-width: 100%;
  }
  tr, img { page-break-inside: avoid; }
}
@media only screen and (min-width: 992px) {
  pre { white-space: pre; }
}
</style>
</head>
<body>
<div class="frontmatter">
<div class="title"><h1>mirai - Torch Integration</h1></div>
<div class="author"><h2></h2></div>
<div class="date"><h3></h3></div>
</div>
<div class="body">
<h3 id="torch-integration">Torch Integration</h3>
<p>Custom serialization functions may be registered to handle external pointer type reference objects.</p>
<p>This allows tensors from the <a href="https://torch.mlverse.org/"><code>torch</code></a> package to be used seamlessly in ‘mirai’ computations.</p>
<h4 id="setup-steps">Setup Steps</h4>
<ol>
<li>
<p>Register the serialization and unserialization functions as a list supplied to <code>serialization()</code>, specifying ‘class’ as ‘torch_tensor’ and ‘vec’ as TRUE.</p>
</li>
<li>
<p>Set up dameons - this may be done before or after setting <code>serialization()</code>.</p>
</li>
<li>
<p>Use <code>everywhere()</code> to make the <code>torch</code> package available on all daemons for convenience (optional).</p>
</li>
</ol>
<pre><code class="language-r">library(mirai)
library(torch)

serialization(refhook = list(torch:::torch_serialize, torch::torch_load),
              class = &quot;torch_tensor&quot;,
              vec = TRUE)
daemons(1)
#&gt; [1] 1
everywhere(library(torch))
</code></pre>
<h4 id="example-usage">Example Usage</h4>
<p>The below example creates a convolutional neural network using <code>torch::nn_module()</code>.</p>
<p>A set of model parameters is also specified.</p>
<p>The model specification and parameters are then passed to and initialized within a ‘mirai’.</p>
<pre><code class="language-r">model &lt;- nn_module(
  initialize = function(in_size, out_size) {
    self$conv1 &lt;- nn_conv2d(in_size, out_size, 5)
    self$conv2 &lt;- nn_conv2d(in_size, out_size, 5)
  },
  forward = function(x) {
    x &lt;- self$conv1(x)
    x &lt;- nnf_relu(x)
    x &lt;- self$conv2(x)
    x &lt;- nnf_relu(x)
    x
  }
)

params &lt;- list(in_size = 1, out_size = 20)

m &lt;- mirai(do.call(model, params), model = model, params = params)

call_mirai(m)$data
#&gt; An `nn_module` containing 1,040 parameters.
#&gt; 
#&gt; ── Modules ─────────────────────────────────────────────────────────────────────────────────────────────────────────────────
#&gt; • conv1: &lt;nn_conv2d&gt; #520 parameters
#&gt; • conv2: &lt;nn_conv2d&gt; #520 parameters
</code></pre>
<p>The returned model is an object containing many tensor elements.</p>
<pre><code class="language-r">m$data$parameters$conv1.weight
#&gt; torch_tensor
#&gt; (1,1,.,.) = 
#&gt;   0.1682  0.1571  0.0880  0.1221 -0.0901
#&gt;   0.1598  0.0815  0.1215  0.0138 -0.1608
#&gt;  -0.0636  0.1779 -0.0090 -0.0544  0.0573
#&gt;   0.0618 -0.1297  0.1649 -0.0272 -0.1092
#&gt;  -0.1108  0.0749 -0.1585 -0.1256  0.1463
#&gt; 
#&gt; (2,1,.,.) = 
#&gt;   0.1313 -0.1575 -0.0257  0.1861 -0.0727
#&gt;  -0.1925  0.1271  0.0439 -0.0086 -0.1037
#&gt;   0.0715  0.0691  0.1181 -0.1048  0.0228
#&gt;   0.0806  0.1871 -0.0866 -0.1398  0.1804
#&gt;   0.1763 -0.1477 -0.1943  0.1088 -0.1668
#&gt; 
#&gt; (3,1,.,.) = 
#&gt;   0.1692  0.1886  0.0159 -0.0523 -0.1667
#&gt;  -0.0769  0.1226 -0.1706  0.0817 -0.1353
#&gt;  -0.1113 -0.0852  0.0073 -0.0749 -0.1621
#&gt;   0.0164 -0.0800  0.1728 -0.1919  0.1623
#&gt;   0.0064 -0.0127  0.0406  0.1909 -0.0757
#&gt; 
#&gt; (4,1,.,.) = 
#&gt;   0.1336 -0.0816 -0.1282 -0.0812  0.0911
#&gt;  -0.1254 -0.1583 -0.1503 -0.1723  0.0713
#&gt;  -0.0995  0.0038  0.1080 -0.0095 -0.1887
#&gt;   0.0842  0.0617 -0.0039 -0.0643 -0.0611
#&gt;  -0.1795  0.0070 -0.0341 -0.0909 -0.1070
#&gt; 
#&gt; (5,1,.,.) = 
#&gt;   0.0207 -0.0057 -0.1417 -0.0102  0.1322
#&gt; ... [the output was truncated (use n=-1 to disable)]
#&gt; [ CPUFloatType{20,1,5,5} ][ requires_grad = TRUE ]
</code></pre>
<p>It is usual for model parameters to then be passed to an optimiser.</p>
<p>This can also be initialized within a ‘mirai’ process.</p>
<pre><code class="language-r">optim &lt;- mirai(optim_rmsprop(params = params), params = m$data$parameters)

call_mirai(optim)$data
#&gt; &lt;optim_rmsprop&gt;
#&gt;   Inherits from: &lt;torch_optimizer&gt;
#&gt;   Public:
#&gt;     add_param_group: function (param_group) 
#&gt;     clone: function (deep = FALSE) 
#&gt;     defaults: list
#&gt;     initialize: function (params, lr = 0.01, alpha = 0.99, eps = 1e-08, weight_decay = 0, 
#&gt;     load_state_dict: function (state_dict, ..., .refer_to_state_dict = FALSE) 
#&gt;     param_groups: list
#&gt;     state: State, R6
#&gt;     state_dict: function () 
#&gt;     step: function (closure = NULL) 
#&gt;     zero_grad: function () 
#&gt;   Private:
#&gt;     step_helper: function (closure, loop_fun)

daemons(0)
#&gt; [1] 0
</code></pre>
<p>Above, tensors and complex objects containing tensors were passed seamlessly between host and daemon processes, in the same way as any other R object.</p>
<p>The custom serialization in <code>mirai</code> leverages R’s own native ‘refhook’ mechanism to allow such completely transparent usage. Designed to be fast and efficient, data copies are minimised and the ‘official’ serialization methods from the <code>torch</code> package are used directly.</p>
</div>
<script src="https://cdn.jsdelivr.net/npm/prismjs@1.29.0/components/prism-core.min.js" defer></script>
<script src="https://cdn.jsdelivr.net/npm/prismjs@1.29.0/plugins/autoloader/prism-autoloader.min.js" defer></script>
</body>
</html>
